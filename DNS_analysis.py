# -*- coding: utf-8 -*-
"""DNS_Query_Log_Analysis (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzPMvqky5Z3lwiXeJ0QbxXIxury_G4VS

# DNS Query Log Analysis with LSTM Autoencoder
"""


"""## Step 1: Load and preprocess the data"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import sys
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed
import matplotlib.pyplot as plt
import json
import base64
from io import BytesIO
import logging
from typing import Dict, Any, Union

# Configure logging with more detailed format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dns_analysis.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def load_and_preprocess_data(file_path: str) -> tuple:
    """Load and preprocess the DNS log data."""
    try:
        data = pd.read_csv(file_path)
        logger.info(f"Successfully loaded data from {file_path}")
        
        missing_values = data.isnull().sum()
        logger.info(f"Missing values in dataset:\n{missing_values}")
        
        data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce')
        data = data.sort_values(by='Timestamp').reset_index(drop=True)
        data['Response Data'].fillna('Unknown', inplace=True)
        
        numeric_columns = ['Response Time (ms)', 'Query Length', 'TTL', 'Source Port']
        numeric_data = data[numeric_columns].dropna()
        
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(numeric_data)
        logger.info(f"Data shape after preprocessing: {scaled_data.shape}")
        
        return data, scaled_data, numeric_columns
    except Exception as e:
        logger.error(f"Error in data preprocessing: {str(e)}")
        raise

def prepare_lstm_data(scaled_data: np.ndarray, timesteps: int = 10) -> np.ndarray:
    """Prepare data for LSTM processing."""
    try:
        X_lstm = []
        for i in range(timesteps, len(scaled_data)):
            X_lstm.append(scaled_data[i-timesteps:i])
        X_lstm = np.array(X_lstm)
        logger.info(f"LSTM input data shape: {X_lstm.shape}")
        return X_lstm
    except Exception as e:
        logger.error(f"Error preparing LSTM data: {str(e)}")
        raise

def build_and_train_model(X_lstm: np.ndarray) -> Sequential:
    """Build and train the LSTM autoencoder model."""
    try:
        model = Sequential([
            LSTM(64, activation='relu', input_shape=(X_lstm.shape[1], X_lstm.shape[2]), 
                 return_sequences=True),
            LSTM(32, activation='relu', return_sequences=False),
            RepeatVector(X_lstm.shape[1]),
            LSTM(32, activation='relu', return_sequences=True),
            LSTM(64, activation='relu', return_sequences=True),
            TimeDistributed(Dense(X_lstm.shape[2]))
        ])
        
        model.compile(optimizer='adam', loss='mse')
        logger.info("Model compiled successfully")
        
        model.fit(X_lstm, X_lstm, epochs=70, batch_size=32, 
                 validation_split=0.2, shuffle=False)
        logger.info("Model training completed")
        
        return model
    except Exception as e:
        logger.error(f"Error in model building/training: {str(e)}")
        raise

def generate_plot(timestamps, mse, low_threshold, moderate_threshold, 
                 severity) -> str:
    """Generate and save the anomaly detection plot."""
    try:
        plt.figure(figsize=(12, 6))
        plt.plot(timestamps, mse, label="Reconstruction Error")
        plt.axhline(y=low_threshold, color="orange", linestyle="--", 
                   label="Low Threshold")
        plt.axhline(y=moderate_threshold, color="red", linestyle="--", 
                   label="Moderate Threshold")
        
        high_anomalies = np.where(np.array(severity) == 'High')[0]
        moderate_anomalies = np.where(np.array(severity) == 'Moderate')[0]

        plt.scatter(timestamps.iloc[high_anomalies], mse[high_anomalies],
                   color='red', marker='x', label="High Severity Anomalies")
        plt.scatter(timestamps.iloc[moderate_anomalies], mse[moderate_anomalies],
                   color='orange', marker='o', label="Moderate Severity Anomalies")

        plt.title("Anomalies and Severity Levels Over Time")
        plt.xlabel("Timestamp")
        plt.ylabel("Reconstruction Error")
        plt.legend()
        
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        plt.close()
        buf.seek(0)
        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
        buf.close()
        
        return image_base64
    except Exception as e:
        logger.error(f"Error generating plot: {str(e)}")
        raise

def analyze_dns_logs(file_path: str) -> Dict[str, Any]:
    """Main function to analyze DNS logs."""
    try:
        # Load and preprocess data
        data, scaled_data, numeric_columns = load_and_preprocess_data(file_path)
        
        # Prepare LSTM data
        X_lstm = prepare_lstm_data(scaled_data)
        
        # Build and train model
        model = build_and_train_model(X_lstm)
        
        # Detect anomalies
        reconstructions = model.predict(X_lstm)
        mse = np.mean(np.square(X_lstm - reconstructions), axis=(1, 2))
        
        # Calculate thresholds
        low_threshold = np.percentile(mse, 80)
        moderate_threshold = np.percentile(mse, 95)
        
        # Classify severity
        severity = ['High' if e > moderate_threshold else 
                   'Moderate' if e > low_threshold else 'Low' 
                   for e in mse]
        
        # Generate plot
        timestamps = data['Timestamp'][10:]  # timesteps = 10
        image_base64 = generate_plot(timestamps, mse, low_threshold, 
                                   moderate_threshold, severity)
        
        # Prepare results
        filtered_data = pd.DataFrame({
            'Timestamp': timestamps,
            'Severity': severity,
            'SourceIp': data['Client IP'][10:]
        })[lambda x: x['Severity'] != 'Low']
        
        num_anomalies = len(filtered_data)
        results = {
                'total_logs': len(data),
                'malicious_events': num_anomalies,
                'alert_level': 'High' if num_anomalies > (len(data)*0.1) else 
                              'Medium' if num_anomalies > (len(data)*0.05) else 'Low',
                "sourceIp": "\n,".join(filtered_data["SourceIp"].tolist()),
                'log_type': "DNS Query Logs",
                'graph_data': image_base64
             }
        
        logger.info("Analysis completed successfully")
        return results
        
    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        raise

def main():
    """Entry point of the script."""
    try:
        if len(sys.argv) != 2:
            raise ValueError("Usage: python DNS_analysis.py <input_file_path>")
        
        input_file = sys.argv[1]
        result = analyze_dns_logs(input_file)
        print(json.dumps(result))
        
    except Exception as e:
        error_result = {
            "error": str(e),
            "traceback": logging.format_exc()
        }
        print(json.dumps(error_result))
        sys.exit(1)

if __name__ == "__main__":
    main()
